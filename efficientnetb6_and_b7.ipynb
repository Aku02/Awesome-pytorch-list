{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "efficientnetb6-and-b7.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aku02/Awesome-pytorch-list/blob/master/efficientnetb6_and_b7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "trusted": true
      },
      "source": [
        "!pip install -q tensorflow==2.3.0 # Use 2.3.0 for built-in EfficientNet\n",
        "!pip install -q git+https://github.com/keras-team/keras-tuner@master # Use github head for newly added TPU support\n",
        "!pip install -q cloud-tpu-client # Needed for sync TPU version\n",
        "!pip install -U tensorflow-gcs-config==2.3.0 # Needed for using private dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "trusted": true
      },
      "source": [
        "!pip install -q efficientnet\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import efficientnet.tfkeras as efn\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow_addons as tfa\n",
        "!pip install gcsfs\n",
        "from tqdm.notebook import tqdm as tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "trusted": true
      },
      "source": [
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    # Sync TPU version\n",
        "    from cloud_tpu_client import Client\n",
        "    c = Client()\n",
        "    c.configure_tpu_version(tf.__version__, restart_type='ifNeeded')\n",
        "    \n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "    \n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "trusted": true
      },
      "source": [
        "EPOCHS = 18\n",
        "BATCH_SIZE = 4 * strategy.num_replicas_in_sync\n",
        "IMAGE_SIZE = [384,384]\n",
        "# Seed\n",
        "SEED = 100\n",
        "# Learning rate\n",
        "LR = 0.0001\n",
        "# Number of classes\n",
        "NUMBER_OF_CLASSES = 81313"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "trusted": true
      },
      "source": [
        "# Seed everything\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "# Function to decode our images (normalize and reshape)\n",
        "def decode_image(image_data):\n",
        "    image = tf.image.decode_jpeg(image_data, channels = 3)\n",
        "    # Convert image to floats in [0, 1] range\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image\n",
        "\n",
        "# This function parse our images and also get the target variable\n",
        "def read_tfrecord(example):\n",
        "    TFREC_FORMAT = {\n",
        "        # tf.string means bytestring\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), \n",
        "        # shape [] means single element\n",
        "        \"label\": tf.io.FixedLenFeature([], tf.int64)\n",
        "        }\n",
        "    example = tf.io.parse_single_example(example, TFREC_FORMAT)\n",
        "    image = decode_image(example['image'])\n",
        "    image=tf.reverse(image, axis=[-1])\n",
        "    target = tf.cast(example['label'], tf.int32)\n",
        "    return image, target\n",
        "\n",
        "# This function load our tf records and parse our data with the previous function\n",
        "def load_dataset(filenames, ordered = False):\n",
        "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
        "    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n",
        "    \n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        # Disable order, increase speed\n",
        "        ignore_order.experimental_deterministic = False \n",
        "        \n",
        "    # Automatically interleaves reads from multiple files\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "    # Use data as soon as it streams in, rather than in its original order\n",
        "    dataset = dataset.with_options(ignore_order)\n",
        "    # Returns a dataset of (image, label) pairs\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls = AUTO) \n",
        "    return dataset\n",
        "\n",
        "# This function output the data so that we can use arcface\n",
        "def arcface_format(image, target):\n",
        "    image=tf.image.resize(image,[384,384])\n",
        "    image=tf.reverse(image, axis=[-1])\n",
        "    return {'inp1': image, 'inp2': target}, target\n",
        "\n",
        "# Training data pipeline\n",
        "def get_training_dataset(filenames, ordered = False):\n",
        "    dataset = load_dataset(filenames, ordered = ordered)\n",
        "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
        "    # The training dataset must repeat for several epochs\n",
        "    dataset = dataset.repeat() \n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    # Prefetch next batch while training (autotune prefetch buffer size)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "# Validation data pipeline\n",
        "def get_validation_dataset(filenames, ordered = True, prediction = False):\n",
        "    dataset = load_dataset(filenames, ordered = ordered)\n",
        "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
        "    # If we are in prediction mode, use bigger batch size for faster prediction\n",
        "    if prediction:\n",
        "        dataset = dataset.batch(BATCH_SIZE * 4)\n",
        "    else:\n",
        "        dataset = dataset.batch(BATCH_SIZE)\n",
        "    # Prefetch next batch while training (autotune prefetch buffer size)\n",
        "    dataset = dataset.prefetch(AUTO) \n",
        "    return dataset\n",
        "\n",
        "\n",
        "\n",
        "# Function for a custom learning rate scheduler with warmup and decay\n",
        "def get_lr_callback():\n",
        "    lr_start   = 0.000001\n",
        "    lr_max     = 0.0000005 * BATCH_SIZE\n",
        "    lr_min     = 0.000001\n",
        "    lr_ramp_ep = 5\n",
        "    lr_sus_ep  = 0\n",
        "    lr_decay   = 0.8\n",
        "   \n",
        "    def lrfn(epoch):\n",
        "        epoch=5\n",
        "        if epoch < lr_ramp_ep:\n",
        "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
        "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
        "            lr = lr_max    \n",
        "        else:\n",
        "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
        "        return lr\n",
        "\n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = False)\n",
        "    return lr_callback\n",
        "\n",
        "# Function to calculate global average precision score\n",
        "def gap_vector(pred, conf, true, return_x = False):\n",
        "    '''\n",
        "    Compute Global Average Precision (aka micro AP), the metric for the\n",
        "    Google Landmark Recognition competition. \n",
        "    This function takes predictions, labels and confidence scores as vectors.\n",
        "    In both predictions and ground-truth, use None/np.nan for \"no label\".\n",
        "\n",
        "    Args:\n",
        "        pred: vector of integer-coded predictions\n",
        "        conf: vector of probability or confidence scores for pred\n",
        "        true: vector of integer-coded labels for ground truth\n",
        "        return_x: also return the data frame used in the calculation\n",
        "\n",
        "    Returns:\n",
        "        GAP score\n",
        "    '''\n",
        "    x = pd.DataFrame({'pred': pred, 'conf': conf, 'true': true})\n",
        "    x.sort_values('conf', ascending = False, inplace = True, na_position = 'last')\n",
        "    x['correct'] = (x.true == x.pred).astype(int)\n",
        "    x['prec_k'] = x.correct.cumsum() / (np.arange(len(x)) + 1)\n",
        "    x['term'] = x.prec_k * x.correct\n",
        "    gap = x.term.sum() / x.true.count()\n",
        "    if return_x:\n",
        "        return gap, x\n",
        "    else:\n",
        "        return gap\n",
        "\n",
        "class ArcMarginProduct(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Implements large margin arc distance.\n",
        "\n",
        "    Reference:\n",
        "        https://arxiv.org/pdf/1801.07698.pdf\n",
        "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
        "            blob/master/src/modeling/metric_learning.py\n",
        "    '''\n",
        "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
        "                 ls_eps=0.0, **kwargs):\n",
        "\n",
        "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.ls_eps = ls_eps\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = tf.math.cos(m)\n",
        "        self.sin_m = tf.math.sin(m)\n",
        "        self.th = tf.math.cos(math.pi - m)\n",
        "        self.mm = tf.math.sin(math.pi - m) * m\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'n_classes': self.n_classes,\n",
        "            's': self.s,\n",
        "            'm': self.m,\n",
        "            'ls_eps': self.ls_eps,\n",
        "            'easy_margin': self.easy_margin,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ArcMarginProduct, self).build(input_shape[0])\n",
        "\n",
        "        self.W = self.add_weight(\n",
        "            name='W',\n",
        "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
        "            initializer='glorot_uniform',\n",
        "            dtype='float32',\n",
        "            trainable=True,\n",
        "            regularizer=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X, y = inputs\n",
        "        y = tf.cast(y, dtype=tf.int32)\n",
        "        cosine = tf.matmul(\n",
        "            tf.math.l2_normalize(X, axis=1),\n",
        "            tf.math.l2_normalize(self.W, axis=0)\n",
        "        )\n",
        "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = tf.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        one_hot = tf.cast(\n",
        "            tf.one_hot(y, depth=self.n_classes),\n",
        "            dtype=cosine.dtype\n",
        "        )\n",
        "        if self.ls_eps > 0:\n",
        "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
        "\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        return output\n",
        "\n",
        "def generalized_mean_pool_2d(X):\n",
        "    gm_exp = 3.0\n",
        "    pool = (tf.reduce_mean(tf.abs(X**(gm_exp)), \n",
        "                        axis = [1, 2], \n",
        "                        keepdims = False) + 1.e-7)**(1./gm_exp)\n",
        "    return pool\n",
        "# Function to build our model using fine tunning (efficientnet)\n",
        "def batch_gap(y_t, y_p):\n",
        "    pred_cat = tf.argmax(y_p, axis=-1)    \n",
        "    y_t_cat = tf.argmax(y_t, axis=-1) * tf.cast(\n",
        "        tf.reduce_sum(y_t, axis=-1), tf.int64)\n",
        "    \n",
        "    n_pred = tf.shape(pred_cat)[0]\n",
        "    is_c = tf.cast(tf.equal(pred_cat, y_t_cat), tf.float32)\n",
        "\n",
        "    GAP = tf.reduce_mean(\n",
        "          tf.cumsum(is_c) * is_c / tf.cast(\n",
        "              tf.range(1, n_pred + 1), \n",
        "              dtype=tf.float32))\n",
        "    \n",
        "    return GAP\n",
        "def generalized_mean_pool_2d(X):\n",
        "    gm_exp = 3.0\n",
        "    pool = (tf.reduce_mean(tf.abs(X**(gm_exp)), \n",
        "                        axis = [1, 2], \n",
        "                        keepdims = False) + 1.e-7)**(1./gm_exp)\n",
        "    return pool\n",
        "def get_model():\n",
        "\n",
        "    with strategy.scope():\n",
        "\n",
        "        margin = ArcMarginProduct(\n",
        "            n_classes = NUMBER_OF_CLASSES, \n",
        "            s = 64, \n",
        "            m = 0.05, \n",
        "            name='head/arc_margin', \n",
        "            dtype='float32'\n",
        "            )\n",
        "\n",
        "        inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n",
        "        label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
        "        x0 = efn.EfficientNetB6(weights = 'imagenet', include_top = False)(inp)\n",
        "        x = tf.keras.layers.GlobalAveragePooling2D()(x0)\n",
        "        x = tf.keras.layers.Dropout(0.356)(x)\n",
        "        x = tf.keras.layers.Dense(512)(x)\n",
        "        x = margin([x, label])\n",
        "        \n",
        "        output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
        "\n",
        "        model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n",
        "\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate = LR)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer = opt,\n",
        "            loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
        "            metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "            ) \n",
        "        \n",
        "        return model\n",
        "\n",
        "\n",
        "# Count the number of observations with the tabular csv\n",
        "def count_data_items(filenames):\n",
        "    records = [int(filename.split('-')[-1].split('.')[0]) for filename in filenames]\n",
        "    return sum(records)\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
        "\n",
        "# For tf.dataset\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Data access\n",
        "GCS_PATH_1 ='gs://kds-4c1ac9004efd00b8a8bcfa9b3dcc800dbdf8978d612363a73d982469' \n",
        "GCS_PATH_2 ='gs://kds-7850820fffb4d34978de60497a6b764639bd2c8ff0405571fcdb6582'\n",
        "DICT_PATH = '../input/landmark-image-train/train_encoded.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oK6LrOyajaa",
        "trusted": true
      },
      "source": [
        "# Configuration\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32 \n",
        "IMAGE_SIZE = [384, 384]\n",
        "# Seed\n",
        "SEED = 100\n",
        "# Learning rate\n",
        "LR = 0.0001\n",
        "# Number of classes\n",
        "NUMBER_OF_CLASSES = 81313\n",
        "\n",
        "# Training filenames directory\n",
        "FILENAMES = tf.io.gfile.glob(GCS_PATH_1 + '/train*.tfrec') + tf.io.gfile.glob(GCS_PATH_2 + '/train*.tfrec')\n",
        "# Read csv file\n",
        "df = pd.read_csv(DICT_PATH)\n",
        "# Using 20% of the data to validate\n",
        "TRAINING_FILENAMES, VALIDATION_FILENAMES = train_test_split(FILENAMES, test_size = 0.20, random_state = SEED)\n",
        "training_groups = [int(re.compile(r\"_([0-9]*)\\.\").search(filename).group(1)) for filename in TRAINING_FILENAMES]\n",
        "validation_groups = [int(re.compile(r\"_([0-9]*)\\.\").search(filename).group(1)) for filename in VALIDATION_FILENAMES]\n",
        "n_trn_classes = df[df['group'].isin(training_groups)]['landmark_id_encode'].nunique()\n",
        "n_val_classes = df[df['group'].isin(validation_groups)]['landmark_id_encode'].nunique()\n",
        "print(f'The number of unique training classes is {n_trn_classes} of {NUMBER_OF_CLASSES} total classes')\n",
        "print(f'The number of unique validation classes is {n_val_classes} of {NUMBER_OF_CLASSES} total classes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F0Z5qsQat_4",
        "trusted": true
      },
      "source": [
        "# Seed everything\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "# Function to decode our images (normalize and reshape)\n",
        "def decode_image(image_data):\n",
        "    image = tf.image.decode_jpeg(image_data, channels = 3)\n",
        "    # Convert image to floats in [0, 1] range\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    # Explicit size needed for TPU\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
        "    return image\n",
        "\n",
        "# This function parse our images and also get the target variable\n",
        "def read_tfrecord(example):\n",
        "    TFREC_FORMAT = {\n",
        "        # tf.string means bytestring\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), \n",
        "        # shape [] means single element\n",
        "        \"target\": tf.io.FixedLenFeature([], tf.int64)\n",
        "        }\n",
        "    example = tf.io.parse_single_example(example, TFREC_FORMAT)\n",
        "    image = decode_image(example['image'])\n",
        "    target = tf.cast(example['target'], tf.int32)\n",
        "    return image, target\n",
        "\n",
        "# This function load our tf records and parse our data with the previous function\n",
        "def load_dataset(filenames, ordered = False):\n",
        "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
        "    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n",
        "    \n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        # Disable order, increase speed\n",
        "        ignore_order.experimental_deterministic = False \n",
        "        \n",
        "    # Automatically interleaves reads from multiple files\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "    # Use data as soon as it streams in, rather than in its original order\n",
        "    dataset = dataset.with_options(ignore_order)\n",
        "    # Returns a dataset of (image, label) pairs\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls = AUTO) \n",
        "    return dataset\n",
        "\n",
        "# This function output the data so that we can use arcface\n",
        "def arcface_format(image, target):\n",
        "    return {'inp1': image, 'inp2': target}, target\n",
        "\n",
        "# Training data pipeline\n",
        "def get_training_dataset(filenames, ordered = False):\n",
        "    dataset = load_dataset(filenames, ordered = ordered)\n",
        "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
        "    # The training dataset must repeat for several epochs\n",
        "    dataset = dataset.repeat() \n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    # Prefetch next batch while training (autotune prefetch buffer size)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "# Validation data pipeline\n",
        "def get_validation_dataset(filenames, ordered = True, prediction = False):\n",
        "    dataset = load_dataset(filenames, ordered = ordered)\n",
        "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
        "    # If we are in prediction mode, use bigger batch size for faster prediction\n",
        "    if prediction:\n",
        "        dataset = dataset.batch(BATCH_SIZE * 4)\n",
        "    else:\n",
        "        dataset = dataset.batch(BATCH_SIZE)\n",
        "    # Prefetch next batch while training (autotune prefetch buffer size)\n",
        "    dataset = dataset.prefetch(AUTO) \n",
        "    return dataset\n",
        "\n",
        "# Count the number of observations with the tabular csv\n",
        "def count_data_items(filenames):\n",
        "    records = [int(re.compile(r\"_([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    df = pd.read_csv(DICT_PATH)\n",
        "    n = df[df['group'].isin(records)].shape[0]\n",
        "    return n\n",
        "\n",
        "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
        "NUM_VALIDATION_IMAGES  = count_data_items(VALIDATION_FILENAMES)\n",
        "print(f'Training with {NUM_TRAINING_IMAGES} images')\n",
        "print(f'Validating with {NUM_VALIDATION_IMAGES} images')\n",
        "\n",
        "# Function for a custom learning rate scheduler with warmup and decay\n",
        "def get_lr_callback():\n",
        "    lr_start   = 0.000001\n",
        "    lr_max     = 0.0000005 * BATCH_SIZE\n",
        "    lr_min     = 0.000001\n",
        "    lr_ramp_ep = 5\n",
        "    lr_sus_ep  = 0\n",
        "    lr_decay   = 0.8\n",
        "   \n",
        "    def lrfn(epoch):\n",
        "        if epoch < lr_ramp_ep:\n",
        "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
        "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
        "            lr = lr_max    \n",
        "        else:\n",
        "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
        "        return lr\n",
        "\n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = False)\n",
        "    return lr_callback\n",
        "\n",
        "# Function to calculate global average precision score\n",
        "def gap_vector(pred, conf, true, return_x = False):\n",
        "    '''\n",
        "    Compute Global Average Precision (aka micro AP), the metric for the\n",
        "    Google Landmark Recognition competition. \n",
        "    This function takes predictions, labels and confidence scores as vectors.\n",
        "    In both predictions and ground-truth, use None/np.nan for \"no label\".\n",
        "\n",
        "    Args:\n",
        "        pred: vector of integer-coded predictions\n",
        "        conf: vector of probability or confidence scores for pred\n",
        "        true: vector of integer-coded labels for ground truth\n",
        "        return_x: also return the data frame used in the calculation\n",
        "\n",
        "    Returns:\n",
        "        GAP score\n",
        "    '''\n",
        "    x = pd.DataFrame({'pred': pred, 'conf': conf, 'true': true})\n",
        "    x.sort_values('conf', ascending = False, inplace = True, na_position = 'last')\n",
        "    x['correct'] = (x.true == x.pred).astype(int)\n",
        "    x['prec_k'] = x.correct.cumsum() / (np.arange(len(x)) + 1)\n",
        "    x['term'] = x.prec_k * x.correct\n",
        "    gap = x.term.sum() / x.true.count()\n",
        "    if return_x:\n",
        "        return gap, x\n",
        "    else:\n",
        "        return gap\n",
        "\n",
        "class ArcMarginProduct(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Implements large margin arc distance.\n",
        "\n",
        "    Reference:\n",
        "        https://arxiv.org/pdf/1801.07698.pdf\n",
        "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
        "            blob/master/src/modeling/metric_learning.py\n",
        "    '''\n",
        "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
        "                 ls_eps=0.0, **kwargs):\n",
        "\n",
        "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.ls_eps = ls_eps\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = tf.math.cos(m)\n",
        "        self.sin_m = tf.math.sin(m)\n",
        "        self.th = tf.math.cos(math.pi - m)\n",
        "        self.mm = tf.math.sin(math.pi - m) * m\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'n_classes': self.n_classes,\n",
        "            's': self.s,\n",
        "            'm': self.m,\n",
        "            'ls_eps': self.ls_eps,\n",
        "            'easy_margin': self.easy_margin,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ArcMarginProduct, self).build(input_shape[0])\n",
        "\n",
        "        self.W = self.add_weight(\n",
        "            name='W',\n",
        "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
        "            initializer='glorot_uniform',\n",
        "            dtype='float32',\n",
        "            trainable=True,\n",
        "            regularizer=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X, y = inputs\n",
        "        y = tf.cast(y, dtype=tf.int32)\n",
        "        cosine = tf.matmul(\n",
        "            tf.math.l2_normalize(X, axis=1),\n",
        "            tf.math.l2_normalize(self.W, axis=0)\n",
        "        )\n",
        "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = tf.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        one_hot = tf.cast(\n",
        "            tf.one_hot(y, depth=self.n_classes),\n",
        "            dtype=cosine.dtype\n",
        "        )\n",
        "        if self.ls_eps > 0:\n",
        "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
        "\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        return output\n",
        "\n",
        "\n",
        "# Function to build our model using fine tunning (efficientnet)\n",
        "def get_model():\n",
        "\n",
        "    with strategy.scope():\n",
        "\n",
        "        margin = ArcMarginProduct(\n",
        "            n_classes = NUMBER_OF_CLASSES, \n",
        "            s = 64, \n",
        "            m = 0.05, \n",
        "            name='head/arc_margin', \n",
        "            dtype='float32'\n",
        "            )\n",
        "\n",
        "        inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n",
        "        label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
        "        x0 = efn.EfficientNetB6(weights = 'imagenet', include_top = False)(inp)\n",
        "        x = tf.keras.layers.GlobalAveragePooling2D()(x0)\n",
        "        x = tf.keras.layers.Dropout(0.356)(x)\n",
        "        x = tf.keras.layers.Dense(512)(x)\n",
        "        x = margin([x, label])\n",
        "        \n",
        "        output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
        "\n",
        "        model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n",
        "\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate = LR)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer = opt,\n",
        "            loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
        "            metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "            ) \n",
        "        \n",
        "        return model\n",
        "\n",
        "# Seed everything\n",
        "seed_everything(SEED)\n",
        "\n",
        "# Build training and validation generators\n",
        "train_dataset = get_training_dataset(TRAINING_FILENAMES, ordered = False)\n",
        "val_dataset = get_validation_dataset(VALIDATION_FILENAMES, ordered = True, prediction = False)\n",
        "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
        "\n",
        "model = get_model()\n",
        "# Using a checkpoint to save best model (want the entire model, not only the weights)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(f'./baseline_model_effb6_arcface.h5', \n",
        "                                                 monitor = 'val_loss', \n",
        "                                                 save_best_only = True, \n",
        "                                                 save_weights_only = False)\n",
        "# Using learning rate scheduler\n",
        "cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', \n",
        "                                                       mode = 'min', \n",
        "                                                       factor = 0.5, \n",
        "                                                       patience = 1, \n",
        "                                                       verbose = 1, \n",
        "                                                       min_delta = 0.0001)\n",
        "\n",
        "# Train and evaluate our model\n",
        "history = model.fit(train_dataset,  \n",
        "                    steps_per_epoch = STEPS_PER_EPOCH,\n",
        "                    epochs = 1,\n",
        "                    callbacks = [get_lr_callback(), checkpoint],\n",
        "                    validation_data = val_dataset,\n",
        "                    verbose = 1\n",
        "                    )\n",
        "\n",
        "# Restart tpu\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "# Load best model\n",
        "model = tf.keras.models.load_model('./baseline_model_effb6_arcface.h5')\n",
        "\n",
        "# Reset val dataset, now in prediction mode\n",
        "val_dataset = get_validation_dataset(VALIDATION_FILENAMES, ordered = True, prediction = True)\n",
        "# Get ground truth target for the fold\n",
        "val_target = val_dataset.map(lambda image, target: target).unbatch()\n",
        "val_targets = list(next(iter(val_target.batch(NUM_VALIDATION_IMAGES))).numpy())\n",
        "\n",
        " # Predictions\n",
        "val_image = val_dataset.map(lambda image, target: image['inp1'])\n",
        "# Transform validation dataset as a numpy iterator\n",
        "val_image = val_image.as_numpy_iterator()\n",
        "# Initiate empty list to store predictions and confidences\n",
        "target_predictions = []\n",
        "target_confidences = []\n",
        "# Iterate over validation images and predict in batches of 1024 images\n",
        "batches = math.ceil(NUM_VALIDATION_IMAGES / (BATCH_SIZE * 4))\n",
        "for image in tqdm(val_image, total = batches):\n",
        "    prediction = model.predict(image)\n",
        "    target_prediction = np.argmax(prediction, axis = -1)\n",
        "    target_confidence = np.max(prediction, axis = -1)\n",
        "    target_predictions.extend(list(target_prediction))\n",
        "    target_confidences.extend(list(target_confidence))\n",
        "\n",
        "# Calculate global average precision for the fold\n",
        "gap = gap_vector(target_predictions, target_confidences, val_targets)\n",
        "accuracy_score = metrics.accuracy_score(val_targets, target_predictions)\n",
        "print(f'Our global average precision score is {gap}')\n",
        "print(f'Our accuracy score is {accuracy_score}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hQOG1paHodb",
        "trusted": true
      },
      "source": [
        "model.save_weights('./baseline_model_effb6_arcface.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "trusted": false
      },
      "source": [
        "# # Training filenames directory\n",
        "# FILENAMES_1 = tf.io.gfile.glob(GCS_PATH_1 + '/train*.tfrec') \n",
        "# FILENAMES_2 = tf.io.gfile.glob(GCS_PATH_2 + '/train*.tfrec')\n",
        "# FILENAMES = FILENAMES_1 + FILENAMES_2\n",
        "# # Read csv file\n",
        "# df = pd.read_csv('/content/gdrive/My Drive/Kaggle/train.csv')\n",
        "# # Using 20% of the data to validate\n",
        "# TRAINING_FILENAMES, VALIDATION_FILENAMES = train_test_split(FILENAMES, test_size = 0.20, random_state = SEED)\n",
        "# TRAINING_FILENAMES=TRAINING_FILENAMES[9:]\n",
        "# NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
        "# NUM_VALIDATION_IMAGES  = count_data_items(VALIDATION_FILENAMES)\n",
        "# print(f'Training with {NUM_TRAINING_IMAGES} images')\n",
        "# print(f'Validating with {NUM_VALIDATION_IMAGES} images')\n",
        "# # Seed everything\n",
        "# seed_everything(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhwtNqDRmcop",
        "trusted": false
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}